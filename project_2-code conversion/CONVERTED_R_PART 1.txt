import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the data (replace 'mcdonalds.csv' with your actual data path)
data = pd.read_csv("mcdonalds.csv")

# Get column names and data dimensions
print(data.columns)
print(data.shape)

# Print the first 3 rows
print(data.head(3))

# Select the first 11 columns and convert categorical data to numerical
MD_x = data.iloc[:, :11]
le = LabelEncoder()
MD_x = le.fit_transform(MD_x)

# Calculate column means (round to 2 decimals)
print(np.round(MD_x.mean(axis=0), 2))

# Perform PCA with sklearn.decomposition.PCA
pca = PCA()
MD_pca = pca.fit_transform(MD_x)

# Print summary of PCA (use 'pca.explained_variance_ratio_' for explained variance)
print(pca.explained_variance_ratio_)

# Print details of PCA (limited to 1 decimal place)
print(pca.components_)

# Plot the data using PCA components (assuming 2 components)
plt.scatter(MD_pca[:, 0], MD_pca[:, 1], c='grey')

# Project original axes onto the PCA space for reference
plt.arrow(0, 0, pca.components_[0, 0], pca.components_[0, 1], color='red', label='PC1')
plt.arrow(0, 0, pca.components_[1, 0], pca.components_[1, 1], color='blue', label='PC2')

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

# K-means clustering example (replace n_clusters with your desired number)
kmeans = KMeans(n_clusters=3)
kmeans.fit(MD_pca)
plt.scatter(MD_pca[:, 0], MD_pca[:, 1], c=kmeans.labels_, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('K-means clustering (3 clusters)')
plt.show()
